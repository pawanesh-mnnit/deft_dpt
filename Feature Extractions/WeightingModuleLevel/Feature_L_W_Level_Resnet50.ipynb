{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d858b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e6d177",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalizationNetwork(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(LocalizationNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_channels, 8, kernel_size=7)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(8, 10, kernel_size=5)\n",
    "\n",
    "        # Placeholder fc1 (will update after shape calculation)\n",
    "        self.fc1 = nn.Linear(1, 32)  # Temporary placeholder\n",
    "        self.fc2 = nn.Linear(32, 6)  # 6 affine parameters\n",
    "\n",
    "        # Initialize weights for identity transformation\n",
    "        self.fc2.weight.data.zero_()\n",
    "        self.fc2.bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        # Dynamically compute the flattened size\n",
    "        if not hasattr(self, 'computed_fc1'):\n",
    "            flattened_size = x.view(x.shape[0], -1).shape[1]\n",
    "            self.fc1 = nn.Linear(flattened_size, 32).to(x.device)\n",
    "            self.computed_fc1 = True  # Prevent re-initialization\n",
    "\n",
    "        x = x.view(x.shape[0], -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        theta = self.fc2(x)\n",
    "        theta = theta.view(-1, 2, 3)\n",
    "        return theta\n",
    "\n",
    "\n",
    "class WeightingModule(nn.Module):\n",
    "    def __init__(self, sigma=0.5):\n",
    "        super(WeightingModule, self).__init__()\n",
    "        self.lambda_param = nn.Parameter(torch.tensor(0.5))\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def forward(self, grid):\n",
    "        dist2 = grid[..., 0] ** 2 + grid[..., 1] ** 2\n",
    "        weight = 1 + self.lambda_param * torch.exp(-dist2 / (2 * self.sigma ** 2))\n",
    "        return weight.unsqueeze(-1)\n",
    "\n",
    "\n",
    "class DEFTModule(nn.Module):\n",
    "    def __init__(self, input_channels, sigma=0.5):\n",
    "        super(DEFTModule, self).__init__()\n",
    "        self.localization = LocalizationNetwork(input_channels)\n",
    "        self.weighting = WeightingModule(sigma)\n",
    "\n",
    "    def forward(self, x):\n",
    "        theta = self.localization(x)\n",
    "        grid = F.affine_grid(theta, x.size(), align_corners=False)\n",
    "        weight = self.weighting(grid)\n",
    "        x_transformed = F.grid_sample(x, grid, align_corners=False)\n",
    "\n",
    "        if x.shape[1] > 1:\n",
    "            weight = weight.expand(-1, x.shape[2], x.shape[3], x.shape[1]).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            weight = weight.permute(0, 3, 1, 2)\n",
    "\n",
    "        x_weighted = x_transformed * weight\n",
    "        return x_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24516ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PAWANESH\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "resnet_model = resnet50(weights=True)\n",
    "resnet_model = torch.nn.Sequential(*list(resnet_model.children())[:-1])  # Remove classification layer\n",
    "resnet_model.eval().to(device)\n",
    "\n",
    "# Load DEFT Module\n",
    "deft_model = DEFTModule(input_channels=3).to(device)  # Input channels = 3 (RGB)\n",
    "deft_model.eval()\n",
    "\n",
    "###########################################################\n",
    "# Define Image Transformation\n",
    "###########################################################\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb2bc3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Paths for RGB frames & labels\n",
    "# ------------------------------\n",
    "rgb_path = \"D:/Datasets/Datasets/EPIC_Kitchen/RGB/P01_04\"\n",
    "label_csv_path = \"D:/Datasets/Datasets/EPIC_Kitchen/Label/P01_04.csv\"\n",
    "output_csv = \"..Features/Feature_P01_04_tsne.csv\"\n",
    "\n",
    "labels_df = pd.read_csv(label_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2dc5dfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting RGB Features with DEFT: 100%|███████████████████████████████████████████| 6308/6308 [07:00<00:00, 14.98it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extract_features(image_path, transform, device=device):\n",
    "    \"\"\"Extract ResNet50 features from a single image after DEFT transformation.\"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = transform(image).unsqueeze(0).to(device)  # Convert to tensor and send to GPU\n",
    "\n",
    "        # Apply DEFT Transformation\n",
    "        with torch.no_grad():\n",
    "            image = deft_model(image)  # Pass through DEFT Module\n",
    "            features = resnet_model(image).squeeze().cpu().numpy()  # Extract ResNet50 features\n",
    "        \n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping frame: {image_path} due to error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ------------------------------\n",
    "# Sampling Setup\n",
    "# ------------------------------\n",
    "S = 1 # Every 5th frame\n",
    "features_list = []\n",
    "all_frames = sorted(os.listdir(rgb_path))[::S]\n",
    "\n",
    "# ------------------------------\n",
    "# Extract RGB Features Only\n",
    "# ------------------------------\n",
    "for frame in tqdm(all_frames, desc=\"Extracting RGB Features with DEFT\"):\n",
    "    rgb_frame_path = os.path.join(rgb_path, frame)\n",
    "    rgb_features = extract_features(rgb_frame_path, transform, device)\n",
    "\n",
    "    if rgb_features is not None:\n",
    "        frame_number = int(frame.split('_')[-1].split('.')[0])\n",
    "        label_row = labels_df[(labels_df['StartFrame'] <= frame_number) & (labels_df['EndFrame'] >= frame_number)]\n",
    "\n",
    "        if not label_row.empty:\n",
    "            action_class = label_row.iloc[0]['Action_class']\n",
    "        else:\n",
    "            action_class = -1  # Default if no label found\n",
    "\n",
    "        features_list.append([frame, action_class] + rgb_features.tolist())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76e7ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = \"../../Features/Feature_P01_04_tsne.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d570e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Feature extraction completed. Saved to: ../../Features/Feature_P01_04_tsne.csv\n"
     ]
    }
   ],
   "source": [
    "# ----------------- Save to CSV -----------------\n",
    "if len(features_list) == 0:\n",
    "    raise ValueError(\"No valid features extracted. Check paths and formats.\")\n",
    "\n",
    "columns = [\"Frame\", \"Action_class\"] + [f\"Feature_{i}\" for i in range(len(rgb_features))]\n",
    "df = pd.DataFrame(features_list, columns=columns)\n",
    "\n",
    "# ✅ Only create directory if it's not empty\n",
    "output_dir = os.path.dirname(output_csv)\n",
    "if output_dir:\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "df.to_csv(output_csv, index=False)\n",
    "print(f\"\\n✅ Feature extraction completed. Saved to: {output_csv}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
